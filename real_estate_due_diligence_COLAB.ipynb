{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Property Due Diligence Assistant - Google Colab Version\n",
    "### OIDD 2550 - Lab 5: LLM Pitch Project\n",
    "\n",
    "**This notebook runs in Google Colab using cloud-based LLMs instead of local Ollama**\n",
    "\n",
    "---\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **Upload to Google Colab**: File ‚Üí Upload notebook\n",
    "2. **Set Runtime**: Runtime ‚Üí Change runtime type ‚Üí T4 GPU (free tier)\n",
    "3. **Run all cells**: Runtime ‚Üí Run all\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Use HuggingFace Models (Free, No API Key)\n",
    "\n",
    "This uses smaller models that run on Colab's free GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\n!pip install -q transformers accelerate langchain langchain-huggingface chromadb \\\n              pandas numpy matplotlib seaborn plotly folium PyPDF2 python-docx \\\n              sentence-transformers scikit-learn \\\n              opentelemetry-api==1.37.0 \\\n              opentelemetry-sdk==1.37.0 \\\n              opentelemetry-exporter-otlp-proto-common==1.37.0 \\\n              opentelemetry-exporter-otlp-proto-grpc==1.37.0 \\\n              opentelemetry-proto==1.37.0 \\\n              opentelemetry-semantic-conventions==0.58b0\n\nprint(\"‚úÖ Dependencies installed!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"‚úÖ Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Small LLM Model (Phi-2 or TinyLlama)\n",
    "\n",
    "We'll use Microsoft's Phi-2 (2.7B parameters) - it's small enough for Colab free tier but still capable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Phi-2 model (2.7B parameters - fits in Colab free tier)\n",
    "print(\"üîÑ Loading Phi-2 model (this takes 2-3 minutes)...\")\n",
    "\n",
    "model_name = \"microsoft/phi-2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",  # Automatically uses GPU if available\n",
    "    load_in_8bit=True   # Quantize to save memory\n",
    ")\n",
    "\n",
    "# Create text generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.3,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15\n",
    ")\n",
    "\n",
    "# Wrap in LangChain\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print(\"‚úÖ Model loaded and ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Use OpenAI API (Requires API Key)\n",
    "\n",
    "If you have an OpenAI API key, this gives better results than Phi-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 2: Use OpenAI (uncomment if you have API key)\n",
    "\n",
    "# !pip install -q openai langchain-openai\n",
    "\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from google.colab import userdata\n",
    "\n",
    "# # Store your API key in Colab Secrets (left sidebar ‚Üí key icon)\n",
    "# # Name it: OPENAI_API_KEY\n",
    "# api_key = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     model=\"gpt-3.5-turbo\",\n",
    "#     temperature=0.3,\n",
    "#     api_key=api_key\n",
    "# )\n",
    "\n",
    "# print(\"‚úÖ OpenAI LLM ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 3: Use Groq API (Free, Fast)\n",
    "\n",
    "Groq offers free API access to Llama 3.1 70B - faster than Colab GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 3: Use Groq (free, very fast)\n",
    "\n",
    "# !pip install -q groq langchain-groq\n",
    "\n",
    "# from langchain_groq import ChatGroq\n",
    "# from google.colab import userdata\n",
    "\n",
    "# # Get free API key at: https://console.groq.com\n",
    "# # Store in Colab Secrets as: GROQ_API_KEY\n",
    "# api_key = userdata.get('GROQ_API_KEY')\n",
    "\n",
    "# llm = ChatGroq(\n",
    "#     model=\"llama-3.1-8b-instant\",\n",
    "#     temperature=0.3,\n",
    "#     api_key=api_key\n",
    "# )\n",
    "\n",
    "# print(\"‚úÖ Groq LLM ready (Llama 3.1 8B)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Rest of the Notebook (Same as Local Version)\n",
    "\n",
    "Everything below this point works identically to the local version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'embedding_model': 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "    'chunk_size': 1000,\n",
    "    'chunk_overlap': 200,\n",
    "    'vector_db_path': './chroma_db',\n",
    "    'top_k': 5\n",
    "}\n",
    "\n",
    "RISK_WEIGHTS = {\n",
    "    'Structural': 0.30,\n",
    "    'Financial': 0.30,\n",
    "    'Legal': 0.20,\n",
    "    'Operational': 0.10,\n",
    "    'Market': 0.10\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuration set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=CONFIG['embedding_model']\n",
    ")\n",
    "\n",
    "# Text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CONFIG['chunk_size'],\n",
    "    chunk_overlap=CONFIG['chunk_overlap']\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Embeddings ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data (same as local version)\n",
    "SAMPLE_PROPERTY = {\n",
    "    'address': '1234 Oak Street, Austin, TX 78701',\n",
    "    'property_type': 'Multifamily - 4 units',\n",
    "    'asking_price': 425000,\n",
    "    'year_built': 1985,\n",
    "    'sqft': 3200,\n",
    "}\n",
    "\n",
    "SAMPLE_INSPECTION = \"\"\"\n",
    "PROPERTY INSPECTION REPORT\n",
    "\n",
    "CRITICAL ISSUES:\n",
    "1. HVAC System - Unit 2: 18 years old, refrigerant leak. Replacement cost: $5,000-$7,000.\n",
    "2. Roof: 19 years old, multiple damaged shingles, water staining. Remaining life: 2-3 years. Replacement: $12,000-$15,000.\n",
    "\n",
    "MODERATE ISSUES:\n",
    "3. Electrical Panel: Outdated 60 amp service. Upgrade recommended: $3,000-$4,000.\n",
    "4. Water Heaters: Both over 10 years old. Replacement within 2-3 years: $1,200 each.\n",
    "5. Foundation: Minor hairline cracks in southeast corner. Monitor for changes.\n",
    "\n",
    "TOTAL ESTIMATED DEFERRED MAINTENANCE: $25,000-$35,000\n",
    "\"\"\"\n",
    "\n",
    "DOMAIN_KNOWLEDGE = \"\"\"\n",
    "REAL ESTATE VALUATION PRINCIPLES:\n",
    "\n",
    "Cap Rate Analysis:\n",
    "- Cap Rate = NOI / Property Value\n",
    "- Austin multifamily: typically 4.5% - 6.5%\n",
    "\n",
    "Red Flags:\n",
    "1. Structural: Foundation cracks > 1/4 inch, mold, roof leaks, HVAC > 15 years\n",
    "2. Financial: Vacancy > 15%, delinquencies > 30 days, expense ratio > 50%\n",
    "3. Legal: Unpermitted work, zoning violations\n",
    "4. Operational: High turnover, month-to-month leases\n",
    "\n",
    "Renovation Costs (Austin, 2024):\n",
    "- HVAC: $5,000-$8,000 per unit\n",
    "- Roof: $8,000-$15,000\n",
    "- Foundation repair: $2,000-$10,000\n",
    "\"\"\"\n",
    "\n",
    "print(\"‚úÖ Sample data created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create documents\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=SAMPLE_INSPECTION,\n",
    "        metadata={'source': 'inspection', 'type': 'inspection'}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=DOMAIN_KNOWLEDGE,\n",
    "        metadata={'source': 'knowledge', 'type': 'domain'}\n",
    "    )\n",
    "]\n",
    "\n",
    "# Split and create vector store\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=CONFIG['vector_db_path']\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Vector database created with {len(split_docs)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RAG chain\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are a real estate analyst. Use the context below to answer the question.\n",
    "Be specific and cite numbers from the documents.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=PROMPT_TEMPLATE,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": CONFIG['top_k']}),\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG chain ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query\n",
    "test_query = \"What are the critical issues found in the inspection and their costs?\"\n",
    "\n",
    "print(f\"üîç Query: {test_query}\\n\")\n",
    "print(\"ü§ñ Generating response...\\n\")\n",
    "\n",
    "result = qa_chain({\"query\": test_query})\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\"*80)\n",
    "print(result['result'])\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified risk scoring for Colab\n",
    "def calculate_risk_scores():\n",
    "    \"\"\"Calculate risk scores based on property data\"\"\"\n",
    "    \n",
    "    scores = {\n",
    "        'Structural': 35,  # Major issues (roof, HVAC)\n",
    "        'Financial': 55,   # High vacancy\n",
    "        'Legal': 90,       # Clean\n",
    "        'Operational': 75, # Manageable\n",
    "        'Market': 95       # Austin is hot\n",
    "    }\n",
    "    \n",
    "    # Calculate weighted overall\n",
    "    overall = sum(scores[cat] * RISK_WEIGHTS[cat] for cat in scores.keys())\n",
    "    \n",
    "    return scores, round(overall, 1)\n",
    "\n",
    "scores, overall = calculate_risk_scores()\n",
    "\n",
    "print(\"üìä RISK ASSESSMENT REPORT\")\n",
    "print(\"=\"*80)\n",
    "for category, score in scores.items():\n",
    "    weight = RISK_WEIGHTS[category]\n",
    "    print(f\"{category:15} {score:3}/100  (weight: {weight:.0%})\")\n",
    "print(\"=\"*80)\n",
    "print(f\"OVERALL SCORE: {overall}/100\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if overall >= 75:\n",
    "    decision = \"‚úÖ STRONG GO\"\n",
    "elif overall >= 60:\n",
    "    decision = \"‚ö†Ô∏è  PROCEED WITH CAUTION\"\n",
    "else:\n",
    "    decision = \"üî¥ NO GO\"\n",
    "\n",
    "print(f\"\\nDECISION: {decision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk score visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "categories = list(scores.keys())\n",
    "values = list(scores.values())\n",
    "colors = ['#ff4444' if v < 60 else '#ffaa00' if v < 80 else '#44ff44' for v in values]\n",
    "\n",
    "ax.barh(categories, values, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_xlabel('Risk Score (0-100)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Risk Assessment by Category', fontsize=14, fontweight='bold')\n",
    "ax.set_xlim(0, 100)\n",
    "ax.axvline(x=60, color='orange', linestyle='--', alpha=0.5)\n",
    "ax.axvline(x=80, color='green', linestyle='--', alpha=0.5)\n",
    "\n",
    "for i, (cat, score) in enumerate(zip(categories, values)):\n",
    "    ax.text(score + 2, i, f'{score:.0f}', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valuation & Final Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valuation calculation\n",
    "asking_price = 425000\n",
    "deferred_maintenance = 30000\n",
    "fair_value = 395000\n",
    "recommended_offer = fair_value - deferred_maintenance * 0.3\n",
    "\n",
    "print(\"üí∞ VALUATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Asking Price:           ${asking_price:,}\")\n",
    "print(f\"Fair Market Value:      ${fair_value:,}\")\n",
    "print(f\"Deferred Maintenance:   ${deferred_maintenance:,}\")\n",
    "print(f\"Recommended Offer:      ${recommended_offer:,.0f} - ${fair_value:,}\")\n",
    "print(f\"Discount from Ask:      ${asking_price - recommended_offer:,.0f} ({(asking_price - recommended_offer)/asking_price:.1%})\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìã NEGOTIATION STRATEGY:\")\n",
    "print(\"1. Open offer at $385,000\")\n",
    "print(\"2. Request $10,000 seller credit for HVAC replacement\")\n",
    "print(\"3. Budget $25k-35k for deferred maintenance over next 2 years\")\n",
    "print(\"4. Walk-away price: $400,000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This Colab version demonstrates the same AI Property Due Diligence system, but using:\n",
    "- **Cloud GPU** instead of local compute\n",
    "- **HuggingFace models** (Phi-2) instead of Ollama\n",
    "- **Same RAG architecture** (ChromaDB + LangChain)\n",
    "\n",
    "### Advantages of Colab:\n",
    "- ‚úÖ No local setup required\n",
    "- ‚úÖ Free GPU access\n",
    "- ‚úÖ Easy to share with classmates\n",
    "- ‚úÖ Can use APIs (OpenAI, Groq) for better models\n",
    "\n",
    "### Disadvantages:\n",
    "- ‚ùå Smaller models (Phi-2 2.7B vs Llama 3.1 8B)\n",
    "- ‚ùå Session timeout after inactivity\n",
    "- ‚ùå Requires API keys for best performance\n",
    "\n",
    "---\n",
    "\n",
    "**For your presentation, I recommend using the local version (Ollama + Llama 3.1 8B) for best quality outputs. Use this Colab version as a backup or for sharing with your team!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}